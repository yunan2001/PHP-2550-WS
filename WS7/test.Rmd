---
title: "test"
author: "Yunan Chen"
date: "2024-10-17"
output: pdf_document
---

```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(moments))
suppressPackageStartupMessages(library(stats))
library(dplyr)
library(gtsummary)
library(gt)
library(tidyverse)
library(corrplot)
library(RColorBrewer)
library(grDevices)
library(ggcorrplot)
# kidney_df <- read.csv("baseseg.csv")
# kidney_df <- kidney_df %>% 
#   select(c(gfr, bascre, sbase, dbase, baseu, AGE, SEX, black)) %>%
#   rename(sex = SEX, age = AGE) %>%
#   na.omit()
# kidney_df$black <- as.factor(kidney_df$black)
# kidney_df$sex <- as.factor(kidney_df$sex)
```


## Answering Scientific Questions with Regression

Answer the following questions about the difference-in-differences paper you were assigned. (\~1 paragraph per question)

1.  What was the motivating research question? How was this translated to a scientific question and analytic approach?

Previous studies have found that beverage taxes are associated with increased prices and reductions in the volume of beverages sold. However, this tax effect at small, independent stores has not been thoroughly studied. This study was conducted in the purpose to examine whether an increase in excise tax on sweetened beverages was associated with the sustained changes in beverage prices and purchases as well as calories purchased from beverages and high-sugar foods, over 2 years at independent stores in Philadelphia and Baltimore, Maryland. To approach this question, a cross-sectional study was conducted, with Baltimore as the control and Philadelphia as the treated city. A difference-in-difference approach was used to examine the difference in the pretax and posttax beverage prices, fluid ounces purchased, and total calories purchased from beverages and high-sugar foods between the two cities. 


## Model Evaluation Example

These questions are on the paper 'Predicting lung cancer prior to surgical resection in patients with lung nodules' by Deppen et al. This paper introduces a model called TREAT that is currently used in practice to predict lung cancer.

1.  Compare the Mayo model to the TREAT model in terms of the initial goals of building the model, the population the training data represented, the variables included, and the resulting model. (\~2 paragraphs)

The Mayo model focused on improving biopsy screening or referral in the general medical population. The model was designed to evaluate patients with nodules selected from the general population whose lesions were found on imaging. The prevalence of disease in the population considered in the training data is 23%. The model contained six variables: age, smoking history, previous cancer, lesion size, spiculated edge and location.

The TREAT model was constructed based on the need to reduce unnecessary surgery for benign disease and was calibrated to be used in the preoperative evaluation of suspicious lesions in the lung. The model was designed to help surgeons obtain an accurate and well-calibrated predictive model to facilitate the diagnosis of suspected lung cancer without missing early stage disease. The model was trained on people undergoing thoracic surgical evaluation of lung nodules or masses for known or suspected non-small cell lung cancer with a prevalence of 72%. Compared with the Mayo Clinic model, the TREAT model added six variables: gender, body mass index, chronic obstructive pulmonary disease (COPD), lesion growth, FDG-PET positivity, and hemoptysis in preoperative symptoms. For variables related to smoking, The TREAT model included pack-years of smoking, which took into account both duration and intensity of smoking.


The TREAT lung cancer model demonstrated superior performance (AUC = 0.87) compared to the Mayo Clinic model (AUC = 0.80) and was validated in a separate, higher-risk cohort (AUC = 0.89). While the Mayo Clinic model performed well in a Vanderbilt University Medical Center (VUMC) population, its accuracy decreased (AUC = 0.73) as disease prevalence increased in a Veterans Affairs cohort with 95% lung cancer prevalence. The Mayo Clinic model showed poorer calibration, underestimating cancer risk in lower-risk patients, limiting its use in surgical populations with higher cancer prevalence.


2.  What measures or visuals were used to evaluate the models? How do we interpret these? Why do you think these measures were chosen for comparison? (1 paragraph)

The models were evaluated using three key measures: the area under the receiver-operating-characteristic curve (AUC), Brier score, and bootstrapping. AUC was used to assess the model's discrimination ability, or its capacity to differentiate between cancer and benign cases, with higher values indicating better performance. The AUC values were visualized as AUC curves. The Brier score measured model calibration by comparing predicted probabilities with actual outcomes, where lower scores reflect better alignment between predictions and reality. In the box plots that were used to visualize the Brier scores, it can be clearly seen that the TREAT model has lower Brier scores, indicating better calibration compared to the Mayo Clinic model in both cohorts. Bootstrapping was employed to estimate the standard errors of model parameters and predictions, as well as to assess the degree of optimism of the model's accuracy when predicting cancer. The estimated model coefficients were shown in a summary table, along with the odds ratios and p-values. The odds ratios for the TREAT model provided a more intuitive interpretation of the effect size. For example, an OR of 1.05 for age suggests that each additional year increases the odds of lung cancer by 5%. The p-values indicate the statistical significance of each predictor in the TREAT model. A p-value less than 0.05 suggests that the variable is a statistically significant predictor of malignancy.


```{r}
kidney_tbl <- kidney_df %>%
  mutate(black = recode(black, `0` = "Non-Black", `1` ="Black"),
         sex = recode(sex, `0` = "Female", `1` ="Male")) %>%
  tbl_summary(by=black,
              label = list(gfr ~ "Measured glomerular filtration rate",
                           bascre ~ "Base serum Creatinine",
                           sbase ~ "Systolic blood pressure",
                           dbase ~ " Diastolic blood pressure",
                           baseu ~ "Urine protein"
                           ),
              statistic = all_continuous() ~ "{mean} ({sd})") %>%
  modify_spanning_header(update =  all_stat_cols() ~  "**Black**") %>%
  modify_footnote(update = all_stat_cols() ~ "Mean (SD) for continuous; n (%) for categorical") %>%
  bold_labels()
kidney_tbl
```

```{r}
cor_m <-cor(kidney_df[, -c(2, 5, 7,8)])
variable_order <- c("log_baseu", "log_bascre", "dbase", "sbase", "age", "gfr")
r_reordered <- cor_m[variable_order, variable_order]

ggcorrplot(r_reordered, 
           hc.order = TRUE, 
           type = "lower",
           lab = TRUE) +
  ggtitle("Figure 2: Correlation Matrix") +
  theme(plot.title = element_text(hjust = 0.5, size=14),
        axis.text.x = element_text(size = 12),             
        axis.text.y = element_text(size = 12))
```
# 3
```{r}
library(caret)
set.seed(2550)
index <- createDataPartition(kidney_df$gfr, p=0.8, list=FALSE)
train <- kidney_df[index, ]
test <- kidney_df[-index, ]
m1 <- glm(gfr ~ age + log_bascre, family=gaussian, data=train)
predictions <- predict(m1, newdata = test)
mse <- mean((predictions-test$gfr)^2)
mae <- mean(abs(predictions-test$gfr))

cat("The MSE is:", mse, "\n")
cat("The MAE is:", mae, "\n")
```

# 4
```{r}
# Split data by race and calculate performance metrics
test$estimated_gfr <- predict(m1, newdata = test)
performance_by_race <- test %>%
  group_by(black) %>%
  summarise(MSE = mean((gfr - estimated_gfr)^2),
            Bias = mean(estimated_gfr - gfr),
            P10 = mean(abs(estimated_gfr - gfr) / gfr <= 0.10) * 100,
            P30 = mean(abs(estimated_gfr - gfr) / gfr <= 0.30) * 100)
performance_by_race %>%
  kbl(caption = "Performance Metrics by Race") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# Create a scatter plot to compare measured and estimated GFR
ggplot(test, aes(x = gfr, y = estimated_gfr, color = black)) +
  geom_point(alpha=0.8) +
  geom_abline(intercept = 0, slope = 1, linetype = "solid", color = "black", size=1) +  # Perfect prediction line
  geom_smooth(method = "lm", linetype = "dashed", se = FALSE, alpha=0.8) + 
  scale_color_manual(values = c("0" = "red", "1" = "blue"),  # Assign colors manually
                     labels = c("0" = "Non-black", "1" = "Black"),  # Rename legend labels
                     name = "Race") +
  labs(title = "Comparison of Measured and Estimated GFR by Race",
       x = "Measured GFR",
       y = "Estimated GFR") +
  theme_minimal()
```

